{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## === Import Libraries === ##\n",
    "import pysam #count variant alleles from BAM\n",
    "import pandas as pd #data frames\n",
    "import numpy as np #arrays\n",
    "import os #interacting with files\n",
    "from Bio import SeqIO #reading fasta format\n",
    "import re #regular expressions\n",
    "import time\n",
    "import glob\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from scipy.stats import binom\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for reads to include in the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_read(read):\n",
    "    \"\"\"\n",
    "    Helper function to decide what reads should\n",
    "    be keep when parsing alignment file with `pysam`. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    read : AlignedSegment\n",
    "        read from alignment file parsed with `pysam`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True/False if read should be included\n",
    "        \n",
    "    \"\"\"\n",
    "    # Exclude Quality Failures\n",
    "    if read.is_qcfail:\n",
    "        return False\n",
    "    # Exclude Secondary Mappings\n",
    "    if read.is_secondary:\n",
    "        return False\n",
    "    # Exclude Unmapped Reads\n",
    "    if read.is_unmapped:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_sequence(refpath):\n",
    "    \"\"\"\n",
    "    Function to read in the reference sequence as a list.\n",
    "    \"\"\"\n",
    "    return [base.upper() for base in list(SeqIO.parse(refpath, \"fasta\"))[0].seq]\n",
    "\n",
    "\n",
    "def count_coverage(bampath,\n",
    "                   contig, \n",
    "                   refpath, \n",
    "                   callback_function = check_read, \n",
    "                   minimum_QUAL = 25, \n",
    "                   minimum_COV = 100): \n",
    "    \"\"\"\n",
    "    Wrapper around `pysam` count coverage that converts the coverage information into a \n",
    "    pandas dataframe. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Open alignment with pysam\n",
    "    with pysam.AlignmentFile(bampath, \"rb\") as bamfile:\n",
    "        # Get a dataframe of the counts\n",
    "        count_df = pd.DataFrame.from_dict({base:counts for base, counts in zip(\"ACGT\",\n",
    "                                                                               bamfile.count_coverage(contig = contig,\n",
    "                                                                                                      read_callback = callback_function,\n",
    "                                                                                                      quality_threshold = minimum_QUAL))})\n",
    "        # Add the depth at each position\n",
    "        count_df['DP'] = count_df.sum(axis = 1)\n",
    "        # Add the position \n",
    "        count_df['POS'] = count_df.index + 1\n",
    "        # Add the reference allele\n",
    "        count_df['REF'] = get_ref_sequence(refpath)\n",
    "        # Convert counts to frequency \n",
    "        count_df.iloc[:,0:4] = count_df.iloc[:,0:4].div(count_df.DP, axis = 0)\n",
    "        # Handle any NaNs created by dividing by 0 coverage\n",
    "        count_df = count_df.fillna(0)\n",
    "        \n",
    "        return count_df\n",
    "\n",
    "    \n",
    "def coverage_filter(snp_df, threshold = 10): \n",
    "    \"\"\"\n",
    "    Filter rows that don't meet the heuristic \n",
    "    threshold of coverage specified by the user. \n",
    "    \n",
    "    default is set to 10X the reciprocal of the \n",
    "    allele frequency.\n",
    "    \"\"\"\n",
    "    return snp_df.assign(COV_FILTER = lambda df: df.DP >= (1/df.AF * threshold))\n",
    "\n",
    "\n",
    "def observation_filter(snp_df, threshold = 2): \n",
    "    \"\"\"\n",
    "    Filter for the minimum number of reads where a SNP was observed.\n",
    "    \n",
    "    The default is 2 reads containing a SNP.\n",
    "    \"\"\"\n",
    "    return snp_df.assign(OBSV_FILTER = lambda df: df.OBSV >= threshold)\n",
    "\n",
    "\n",
    "def read_position_filter(snp_df, avg_read_length = 71, percentile = 0.9):\n",
    "    \"\"\"\n",
    "    Filter out SNPs that at the extreme start or end of reads. \n",
    "    \"\"\"\n",
    "    maxavg = avg_read_length*percentile\n",
    "    minavg = avg_read_length*(1-percentile)\n",
    "    return snp_df.assign(READPOS_FILTER = lambda df: (df.MEAN_READPOS >= minavg) & (df.MEAN_READPOS <= maxavg))\n",
    "\n",
    "\n",
    "def strand_bias_filter(snp_df, threshold = 0.9):\n",
    "    \"\"\"\n",
    "    Filter out SNPs with a strand bias\n",
    "    \"\"\"\n",
    "    return snp_df.assign(STRANDBIAS_FILTER = lambda df: (df.STRAND_RATIO >= (1-threshold)) & (df.STRAND_RATIO <= threshold))\n",
    "\n",
    "\n",
    "def mask(snp_df, mask = [x for x in range(29860, 29904)]):\n",
    "    \"\"\"\n",
    "    Define a mask of position to exclude in the analysis. \n",
    "    \n",
    "    These could be low complexity, close to the start or \n",
    "    end of the genome, etc.. \n",
    "    \"\"\"\n",
    "    return snp_df.loc[~snp_df.POS.isin(mask)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for making the consensus sequence to call varaints too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_consensus(bampath, refpath, contig, outpath, callback_function = check_read,  minimum_QUAL = 25, minimum_COV = 100):\n",
    "    \"\"\"\n",
    "    Take the count_df, which is output from `count_coverage`, and make a consensus seq as a string. \n",
    "    \n",
    "    Parameters for consensus calling are the minimum quality score, coverage, and the reads that\n",
    "    can be included (marked duplicates, quality fails, etc..) defined by `check_read`. \n",
    "    \n",
    "    Returns consensus as string\n",
    "    \"\"\"\n",
    "    \n",
    "     # String to hold the final consensus seq\n",
    "    consensus = \"\"\n",
    "    \n",
    "    # Make the count dataframe \n",
    "    count_df = count_coverage(bampath = bampath, \n",
    "                               refpath = refpath,\n",
    "                               contig = contig,\n",
    "                               callback_function = callback_function,  \n",
    "                               minimum_QUAL = minimum_QUAL, \n",
    "                               minimum_COV = minimum_COV)\n",
    "    \n",
    "    # Iterate over each row and determine the conesnus sequence and build consensus string\n",
    "    for index, row in count_df.iterrows(): \n",
    "        if row.DP < minimum_COV:\n",
    "            consensus += row.REF\n",
    "        else: \n",
    "            consensus += max([(row[\"A\"], \"A\"), (row[\"C\"], \"C\"), (row[\"G\"], \"G\"), (row[\"T\"], \"T\")],key=lambda x:x[0])[1]\n",
    "\n",
    "    \n",
    "    # Write out a fasta file for phylogenetic analysis\n",
    "    with open(outpath, \"w\") as outfile:\n",
    "        outfile.write(\">\" + contig + \"\\n\" + consensus + \"\\n\")\n",
    "\n",
    "            \n",
    "    #return \"\".join(base for pos, base in enumerate(consensus)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for variant calling to consensus sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_variants(bampath, refpath, contig, callback_function = check_read,  minimum_QUAL = 25, minimum_COV = 100, maxdepth = 500):\n",
    "    \"\"\"\n",
    "    Read in BAM file and convert to a dataframe containing the frequency of \n",
    "    of any bases present at a given position in the reference genome using the\n",
    "    `pysam` command `count_coverage`. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        path to the bam file to be parsed\n",
    "        \n",
    "    callback_function : function\n",
    "        function that decides which reads to keep/exclude\n",
    "        \n",
    "    ref : str\n",
    "        name of the contig to count coverage over\n",
    "        \n",
    "    ref_path : str\n",
    "        path to the reference genome as fasta\n",
    "         \n",
    "    minimum_qual : int\n",
    "        minimum QUAL score to count base observation at a position.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas.DataFrame\n",
    "       Data Frame containing the bases represented at each positon in the genome\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Make the count dataframe \n",
    "    count_df = count_coverage(bampath = bampath, \n",
    "                               refpath = refpath,\n",
    "                               contig = contig,\n",
    "                               callback_function = callback_function,  \n",
    "                               minimum_QUAL = minimum_QUAL, \n",
    "                               minimum_COV = minimum_COV)\n",
    "\n",
    "    # Melt the data frame to a longer ('tidy') form\n",
    "    count_df = pd.melt(count_df, \n",
    "                       id_vars=['POS', 'DP', 'REF'],\n",
    "                       value_vars=[base for base in 'ATGC'],\n",
    "                       value_name='AF',\n",
    "                       var_name='ALT')\n",
    "    \n",
    "    # Remove anything with 0 coverage.\n",
    "    count_df = count_df[count_df['AF'] > 0]\n",
    "    # TRUE/FALSE if it's a SNP\n",
    "    count_df['SNP'] = np.where(count_df['ALT'] != count_df['REF'], True, False)\n",
    "    # TRUE/FALSE if it's a consensus base.\n",
    "    count_df['CONS'] = count_df['AF'].map(lambda x: x >= 0.5)\n",
    "    # Add the number of times a given allele is observed.\n",
    "    count_df['OBSV'] = count_df.DP * count_df.AF\n",
    "    # Filter to only get SNPs\n",
    "    count_df = count_df.loc[count_df['SNP']]\n",
    "\n",
    "    # Merge the pileup dict and the snp dict. \n",
    "    return count_df\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make variant matrix for phasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_variants(SNPs_df, bampath, contig, maxdepth = 500):\n",
    "    \n",
    "    ## ===== Format inputs and get SNP list ===== ##\n",
    "\n",
    "    SNPs_set = set(pair for pair in zip(SNPs_df.POS, SNPs_df.ALT))                   \n",
    "    \n",
    "    ## ===== Move through the BAM file and get haplotypes ===== ##\n",
    "    \n",
    "    # Save the haplotypes in a dictionary\n",
    "    haplotype_dict = {}\n",
    "\n",
    "    # Get the start and stop\n",
    "    start = sorted([pos for pos, alt in SNPs_set])[0]\n",
    "    stop = sorted([pos for pos, alt in SNPs_set])[-1]\n",
    "\n",
    "    # Open alignment with pysam\n",
    "    with pysam.AlignmentFile(bampath, \"rb\") as bamfile:\n",
    "\n",
    "        # Get the pileup column for a specific region\n",
    "        for pileupcolumn in bamfile.pileup(contig, max_depth = maxdepth, start = start, stop = stop, stepper = 'nofilter'):\n",
    "\n",
    "            # Check the if the position has a target SNP (converted to 0-indexed)\n",
    "            if pileupcolumn.pos in [pos-1 for pos, alt in SNPs_set]:\n",
    "\n",
    "                # Iterate over every alignment in the pileup column. \n",
    "                for pileupread in pileupcolumn.pileups:\n",
    "\n",
    "                    # Check if the read is valid and can be parsed\n",
    "                    if check_read(pileupread.alignment) and not pileupread.is_del and not pileupread.is_refskip:\n",
    "\n",
    "                        # Save the query name\n",
    "                        qname = pileupread.alignment.query_name\n",
    "                        \n",
    "                        # Save the 1-indexed position\n",
    "                        pos = pileupcolumn.pos + 1\n",
    "\n",
    "                        # Save the base at that position in the read\n",
    "                        alt = pileupread.alignment.query_sequence[pileupread.query_position]\n",
    "\n",
    "                        # Check if this read has the SNP or not\n",
    "                        if (pos, alt) in SNPs_set:\n",
    "                            phase = 1 # The read has the SNP\n",
    "\n",
    "                        else:\n",
    "                            phase = 0 # The read doesn't have the SNP\n",
    "\n",
    "                        # Add the readname to the dictionary  \n",
    "                        if qname in haplotype_dict.keys():\n",
    "                            haplotype_dict[qname].append((pos, phase, alt)) \n",
    "                        else:\n",
    "                            haplotype_dict[qname] = [(pos, phase, alt)]\n",
    "                     \n",
    "    ## ===== Convert the haplotype dictionary to a dataframe filling in missings ===== ##\n",
    "    \n",
    "    # Save the haplotypes with missing values (not covered by reads) filled\n",
    "    completed_haplotype_dict = {}\n",
    "\n",
    "    # Iterate over the haplotype dictionary created above\n",
    "    for qname, alleles in haplotype_dict.items():\n",
    "        \n",
    "        # Make a dictionary to fill with observed SNPs for each read\n",
    "        aa_dict = {tup:\"-\" for tup in SNPs_set}\n",
    "        \n",
    "        # Iterate over all observed alleles\n",
    "        for allele in alleles:\n",
    "            \n",
    "            # If it's 1, then an allele was observed\n",
    "            if allele[1] == 1:\n",
    "                # Add this observation based on the key\n",
    "                aa_dict[(allele[0], allele[2])] = 1\n",
    "            # If it's a wild type allele \n",
    "            elif allele[1] == 0:\n",
    "                # Check every possible wt allele\n",
    "                for key in aa_dict.keys():\n",
    "                    # Check by position\n",
    "                    if key[0] == allele[0]:\n",
    "                        # If it's the same positon add a 0\n",
    "                        aa_dict[key] = 0\n",
    "\n",
    "        # Add this populated dictionary to the haplotype dictionary\n",
    "        completed_haplotype_dict[qname] = aa_dict\n",
    "\n",
    "    \n",
    "    # Convert this dicitonary into a dataframe and take the transpose\n",
    "    haplotype_df = pd.DataFrame(completed_haplotype_dict).T\n",
    "    \n",
    "    # Replace the '-' with 'NaN'\n",
    "    haplotype_df = haplotype_df.replace('-', np.nan)\n",
    "    \n",
    "    # Return the phased SNP df\n",
    "    return haplotype_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test the probability that haplotype exists based on phase counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def haplotype_probability(haplotype_dict):\n",
    "    \n",
    "#    return (haplotype_dict['01'] * haplotype_dict['10']) / (haplotype_dict['00'] * sum(v for v in haplotype_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate p-value that haplotype exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linked_pvalue(haplotype_dict, t = 0.001):\n",
    "    \n",
    "    o22 = haplotype_dict['11']\n",
    "    \n",
    "    n = sum(v for v in haplotype_dict.values())\n",
    "    p = t\n",
    "    k = o22 - 1\n",
    "    \n",
    "    return 1 - binom.cdf(k,n,p)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate p-value for forbidden pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlinked_pvalue(haplotype_dict, t = 0.001):\n",
    "    \n",
    "    o22 = haplotype_dict['11']\n",
    "    \n",
    "    n = sum(v for v in haplotype_dict.values())\n",
    "    p = t\n",
    "    k = np.arange(0, o22 + 1)\n",
    "\n",
    "    binomial = binom.pmf(k,n,p)\n",
    "    \n",
    "    return sum(binomial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to determine linkage from pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_linkage(haplotype_dict, phase_df, P_forbidden = 0.05, P_linked = 0.05):\n",
    "    \n",
    "    if linked_pvalue(haplotype_dict, t = 0.001) <= (P_linked/special.binom(len(phase_df.columns), 2)): \n",
    "\n",
    "        #print(f\"Position {snps[0][0]} and Position {snps[1][0]} are linked \\n{haplotype_dict}\\n\")\n",
    "        return 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        if unlinked_pvalue(haplotype_dict, t = 0.01) <= (P_forbidden/special.binom(len(phase_df.columns), 2)): \n",
    "\n",
    "            #print(f\"Position {snps[0][0]} and Position {snps[1][0]} are NOT linked \\n{haplotype_dict}\\n\")\n",
    "            return 0\n",
    "\n",
    "        else: \n",
    "\n",
    "            #print(f\"Position {snps[0][0]} and Position {snps[1][0]} are UNDERTERMINED \\n{haplotype_dict}\\n\")\n",
    "            return \"NA\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths \n",
    "refpath = \"../../config/ref/MeVChiTok-SSPE.fa\" \n",
    "bampath = \"../../results/realigned/Frontal_Cortex_2/Frontal_Cortex_2.sorted.bam\"\n",
    "custom_ref = \"./Frontal_Cortex_2.fasta\"\n",
    "\n",
    "# Params\n",
    "callback_function = check_read\n",
    "contig = \"MeVChiTok\" \n",
    "\n",
    "# Coverage \n",
    "minimum_qual = 25\n",
    "minimum_cov = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the consensus sequence \n",
    "make_consensus(bampath = bampath,\n",
    "               refpath = refpath,\n",
    "               contig = contig,\n",
    "               outpath = custom_ref,\n",
    "               callback_function = check_read,\n",
    "               minimum_QUAL = 25,\n",
    "               minimum_COV = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call varaints \n",
    "variant_df = call_variants(bampath = bampath,\n",
    "                            refpath = custom_ref,\n",
    "                            contig = contig,\n",
    "                            minimum_QUAL = minimum_qual, \n",
    "                            minimum_COV = minimum_cov,\n",
    "                            callback_function = check_read, \n",
    "                            maxdepth = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on frequency and minimum number of observations. \n",
    "variant_df = variant_df[(variant_df['OBSV'] > 20) & (variant_df['AF'] > 0.02)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes the phaseing matrix \n",
    "outdf = phase_variants(variant_df, \n",
    "                       bampath,\n",
    "                       contig,\n",
    "                       maxdepth = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>6764</th>\n",
       "      <th>12391</th>\n",
       "      <th>4178</th>\n",
       "      <th>5071</th>\n",
       "      <th>3869</th>\n",
       "      <th>4182</th>\n",
       "      <th>1622</th>\n",
       "      <th>1356</th>\n",
       "      <th>4760</th>\n",
       "      <th>5312</th>\n",
       "      <th>...</th>\n",
       "      <th>10718</th>\n",
       "      <th>4006</th>\n",
       "      <th>8920</th>\n",
       "      <th>1328</th>\n",
       "      <th>4087</th>\n",
       "      <th>4700</th>\n",
       "      <th>5050</th>\n",
       "      <th>13558</th>\n",
       "      <th>1620</th>\n",
       "      <th>7304</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "      <th>A</th>\n",
       "      <th>A</th>\n",
       "      <th>T</th>\n",
       "      <th>C</th>\n",
       "      <th>...</th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>T</th>\n",
       "      <th>C</th>\n",
       "      <th>C</th>\n",
       "      <th>T</th>\n",
       "      <th>C</th>\n",
       "      <th>T</th>\n",
       "      <th>C</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>K00316:387:HHWKYBBXY:4:1126:18639:34741</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K00316:387:HHWKYBBXY:4:1219:27438:26810</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K00316:387:HHWKYBBXY:4:2126:28726:36499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K00316:387:HHWKYBBXY:4:2219:25307:7504</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K00316:387:HHWKYBBXY:4:1121:22993:44007</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        6764  12391 4178  5071  3869  4182   \\\n",
       "                                            A     A     C     A     C     A   \n",
       "K00316:387:HHWKYBBXY:4:1126:18639:34741   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "K00316:387:HHWKYBBXY:4:1219:27438:26810   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "K00316:387:HHWKYBBXY:4:2126:28726:36499   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "K00316:387:HHWKYBBXY:4:2219:25307:7504    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "K00316:387:HHWKYBBXY:4:1121:22993:44007   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "                                        1622  1356  4760  5312   ... 10718  \\\n",
       "                                            A     A     T     C  ...     G   \n",
       "K00316:387:HHWKYBBXY:4:1126:18639:34741   NaN   NaN   NaN   NaN  ...   NaN   \n",
       "K00316:387:HHWKYBBXY:4:1219:27438:26810   NaN   NaN   NaN   NaN  ...   NaN   \n",
       "K00316:387:HHWKYBBXY:4:2126:28726:36499   NaN   NaN   NaN   NaN  ...   NaN   \n",
       "K00316:387:HHWKYBBXY:4:2219:25307:7504    NaN   NaN   NaN   NaN  ...   NaN   \n",
       "K00316:387:HHWKYBBXY:4:1121:22993:44007   NaN   NaN   NaN   NaN  ...   NaN   \n",
       "\n",
       "                                        4006  8920  1328  4087  4700  5050   \\\n",
       "                                            C     T     C     C     T     C   \n",
       "K00316:387:HHWKYBBXY:4:1126:18639:34741   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "K00316:387:HHWKYBBXY:4:1219:27438:26810   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "K00316:387:HHWKYBBXY:4:2126:28726:36499   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "K00316:387:HHWKYBBXY:4:2219:25307:7504    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "K00316:387:HHWKYBBXY:4:1121:22993:44007   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "                                        13558 1620  7304   \n",
       "                                            T     C     C  \n",
       "K00316:387:HHWKYBBXY:4:1126:18639:34741   NaN   NaN   NaN  \n",
       "K00316:387:HHWKYBBXY:4:1219:27438:26810   NaN   NaN   NaN  \n",
       "K00316:387:HHWKYBBXY:4:2126:28726:36499   NaN   NaN   NaN  \n",
       "K00316:387:HHWKYBBXY:4:2219:25307:7504    NaN   NaN   NaN  \n",
       "K00316:387:HHWKYBBXY:4:1121:22993:44007   NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00': 3603, '10': 3618, '01': 435, '11': 16}\n",
      "{'00': 5489, '10': 1028, '01': 203, '11': 12}\n",
      "{'00': 2576, '01': 2197, '10': 890, '11': 14}\n",
      "{'00': 2493, '01': 2171, '10': 874, '11': 15}\n",
      "{'00': 5861, '01': 1212, '10': 146, '11': 11}\n",
      "{'00': 7282, '01': 1363, '10': 288, '11': 16}\n",
      "{'00': 3775, '01': 881, '10': 161, '11': 16}\n",
      "{'00': 5358, '01': 1044, '10': 232, '11': 12}\n",
      "{'00': 7463, '10': 210, '01': 225, '11': 26}\n",
      "{'00': 2355, '01': 2402, '10': 125, '11': 12}\n",
      "{'00': 5976, '01': 888, '10': 198, '11': 10}\n",
      "{'00': 6703, '10': 138, '01': 133, '11': 14}\n",
      "{'00': 5711, '10': 1197, '01': 335, '11': 22}\n",
      "{'00': 5337, '10': 1077, '01': 734, '11': 23}\n",
      "{'10': 2693, '00': 3855, '01': 225, '11': 23}\n",
      "{'10': 3194, '00': 2879, '01': 1062, '11': 17}\n",
      "{'00': 4374, '01': 467, '10': 124, '11': 14}\n",
      "{'00': 4920, '01': 582, '10': 150, '11': 18}\n",
      "{'00': 4586, '01': 509, '10': 134, '11': 16}\n",
      "{'00': 3285, '10': 4271, '01': 912, '11': 11}\n",
      "{'00': 3049, '10': 3928, '01': 862, '11': 14}\n",
      "{'00': 5933, '10': 979, '01': 387, '11': 22}\n",
      "{'00': 4218, '10': 721, '01': 606, '11': 16}\n",
      "{'00': 2287, '01': 2343, '10': 138, '11': 18}\n",
      "{'10': 4675, '01': 3727, '11': 25, '00': 166}\n",
      "{'10': 4688, '01': 3748, '11': 17, '00': 166}\n",
      "{'00': 3114, '01': 3452, '10': 123, '11': 21}\n",
      "{'00': 6017, '01': 268, '10': 113, '11': 12}\n",
      "{'00': 2101, '01': 2638, '10': 541, '11': 19}\n",
      "{'00': 6939, '01': 143, '10': 485, '11': 13}\n",
      "{'01': 975, '10': 2534, '00': 2501, '11': 12}\n",
      "{'01': 1434, '00': 3466, '10': 3885, '11': 25}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-61f790037c40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mreference_genome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ref_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mconsensus_genome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ref_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mREF_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_genome\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPOS_a\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-26139b958334>\u001b[0m in \u001b[0;36mget_ref_sequence\u001b[0;34m(refpath)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mFunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreference\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fasta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-26139b958334>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mFunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreference\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fasta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/Bio/Seq.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Seq API requirement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Seq API requirement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \"\"\"Return a subsequence of single letter, use my_seq[index].\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List to hold the rows\n",
    "row_list = list()\n",
    "# Matrix containing all read information \n",
    "read_matrix = outdf\n",
    "# Maximum distance within which SNPs can be phased\n",
    "max_frag_length = 1000\n",
    "# Minimum Coverage\n",
    "min_cov = 100\n",
    "\n",
    "\n",
    "# All non-redundant comparisons between columns \n",
    "comparisons = list(combinations(outdf.columns.tolist(), 2))\n",
    "\n",
    "\n",
    "# Iterate over the comparisons \n",
    "for snps in comparisons: \n",
    "\n",
    "    # Check that the SNPs are within the allowable distance\n",
    "    if abs(snps[0][0] - snps[1][0]) <= max_frag_length:\n",
    "        # SNPs as list\n",
    "        snps = [snp for snp in snps]\n",
    "        # Slice the matrix to get these two SNPs, removing NaN\n",
    "        phase_df = read_matrix[snps].dropna(thresh=len(snps))\n",
    "        # Check if there are more bridging reads than the minimum required. \n",
    "        if len(phase_df.index) >= min_cov:\n",
    "        \n",
    "            # Create a list of our conditions\n",
    "            conditions = [\n",
    "                (phase_df[snps[0]] == 1) & (phase_df[snps[1]] == 1), #O22\n",
    "                (phase_df[snps[0]] == 0) & (phase_df[snps[1]] == 0), #O11\n",
    "                (phase_df[snps[0]] == 1) & (phase_df[snps[1]] == 0), #O21\n",
    "                (phase_df[snps[0]] == 0) & (phase_df[snps[1]] == 1)  #O12\n",
    "                ]\n",
    "\n",
    "            # Create a list of the values we want to assign for each condition\n",
    "            values = [('11'), ('00'), ('10'), ('01')]\n",
    "\n",
    "            # Create a new column and use np.select to assign values to it using our lists as arguments\n",
    "            phase_df[('haplotype')] = np.select(conditions, values)\n",
    "            \n",
    "            # Make a dictionary of O22, O11, O21, and O12\n",
    "            haplotype_dict = dict(Counter(phase_df.haplotype))\n",
    "            \n",
    "            # Add in the missing values\n",
    "            for k in values: \n",
    "                if k not in haplotype_dict.keys():\n",
    "                    haplotype_dict[k] = 0\n",
    "                    \n",
    "            \n",
    "            if haplotype_dict['11'] >= 10: \n",
    "                linkage = determine_linkage(haplotype_dict, outdf, P_forbidden = 0.05, P_linked = 0.001)\n",
    "            else:\n",
    "                linkage = \"NA\"\n",
    "                \n",
    "            \n",
    "            if linkage == 0:\n",
    "                print(haplotype_dict)\n",
    "            ## ==== ## ==== ## ==== ##\n",
    "            \n",
    "            POS_a = snps[0][0]\n",
    "            POS_b = snps[1][0]\n",
    "            \n",
    "            ALT_a = snps[0][1]\n",
    "            ALT_b = snps[1][1]\n",
    "            \n",
    "            reference_genome = get_ref_sequence(refpath)\n",
    "            consensus_genome = get_ref_sequence(custom_ref)\n",
    "            \n",
    "            REF_a = reference_genome[POS_a - 1]\n",
    "            REF_b = reference_genome[POS_b - 1]\n",
    "            \n",
    "            if REF_a == ALT_a:\n",
    "                \n",
    "                ALT_a = consensus_genome[POS_a - 1]\n",
    "                \n",
    "                if linkage == 1: \n",
    "                    linkage = 0\n",
    "  \n",
    "                elif linkage == 0: \n",
    "                    linkage = 1\n",
    "\n",
    "            \n",
    "            elif REF_b == ALT_b:\n",
    "                \n",
    "                ALT_b = consensus_genome[POS_b - 1]\n",
    "                \n",
    "                if linkage == 1: \n",
    "                    linkage = 0\n",
    "  \n",
    "                elif linkage == 0: \n",
    "                    linkage = 1\n",
    "            \n",
    "            row = [f\"{REF_a}{POS_a}{ALT_a}\", f\"{REF_b}{POS_b}{ALT_b}\", linkage, haplotype_dict['11'], haplotype_dict['10'], haplotype_dict['01'], haplotype_dict['00']]\n",
    "            row_list.append(row)\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(row_list, columns = [\"SNP_a\", \"SNP_b\", \"linkage\", \"O22\", \"O21\" , \"O12\", \"O11\"])\n",
    " \n",
    "# print dataframe.\n",
    "print(df)\n",
    "df.to_csv(\"../../config/linkage.csv\", index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015448542799962176"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unlinked_pvalue({'00': 501, '01': 13, '10': 15, '11': 2}, t = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to labeled haplotypes\n",
    "genomes_path = \"../../config/data/allmutations.csv\" # Path to the major haplotypes\n",
    "\n",
    "# Import the dataframe with mutations labled by identity\n",
    "genomes_df = pd.read_csv(genomes_path)\n",
    "\n",
    "# Get the SNPs known to belong to genome-1 or genome-2\n",
    "g1_snps = set(genomes_df[genomes_df.Genome == \"genome-1\"].SNP)\n",
    "g2_snps = set(genomes_df[genomes_df.Genome == \"genome-2\"].SNP)\n",
    "\n",
    "# SNPs added to the list\n",
    "diff = abs(len(g1_snps.union(g2_snps)))\n",
    "# Iteration\n",
    "iteration = 1\n",
    "\n",
    "# Keep going until no new SNPs are added \n",
    "while diff != 0:\n",
    "    \n",
    "    # Lists to hold the SNPs added to either haplotype\n",
    "    new_g1_snps = list()\n",
    "    new_g2_snps = list()\n",
    "    \n",
    "    print(iteration)\n",
    "    iteration += 1 \n",
    "    \n",
    "    for i in range(len(df)):\n",
    "\n",
    "        SNP_a = df.iloc[i].SNP_a\n",
    "        SNP_b = df.iloc[i].SNP_b\n",
    "        linkage = df.iloc[i].linkage\n",
    "\n",
    "        if SNP_a in g1_snps and SNP_b not in g1_snps.union(g2_snps): \n",
    "            if linkage == 1: \n",
    "                new_g1_snps.append(SNP_b)\n",
    "            elif linkage == 0: \n",
    "                new_g2_snps.append(SNP_b)\n",
    "        elif SNP_a in g2_snps and SNP_b not in g1_snps.union(g2_snps): \n",
    "            if linkage == 1: \n",
    "                new_g2_snps.append(SNP_b)\n",
    "            elif linkage == 0: \n",
    "                new_g1_snps.append(SNP_b)\n",
    "        elif SNP_b in g1_snps and SNP_a not in g1_snps.union(g2_snps): \n",
    "            if linkage == 1: \n",
    "                new_g1_snps.append(SNP_a)\n",
    "            elif linkage == 0: \n",
    "                new_g2_snps.append(SNP_a)\n",
    "        elif SNP_b in g2_snps and SNP_a not in g1_snps.union(g2_snps): \n",
    "            if linkage == 1: \n",
    "                new_g2_snps.append(SNP_a)\n",
    "            elif linkage == 0: \n",
    "                new_g1_snps.append(SNP_a)\n",
    "                \n",
    "    new_g1_snps = set(new_g1_snps)\n",
    "    new_g2_snps = set(new_g2_snps)\n",
    "\n",
    "    diff = abs(len(g1_snps.union(g2_snps)) - len((g1_snps.union(g2_snps)).union((new_g1_snps.union(new_g2_snps)))))\n",
    "    print(f\"Added {diff} new SNPS.\")\n",
    "\n",
    "    g1_snps = g1_snps.union(new_g1_snps)\n",
    "    g2_snps = g2_snps.union(new_g2_snps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([[snp, \"genome-1\"] for snp in g1_snps] + [[snp, \"genome-2\"] for snp in g2_snps],  columns = [\"SNP\", \"Genome\"]).to_csv(\"../../config/gneome_assignments.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the linkage information to make an adjacency matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNP_a</th>\n",
       "      <th>SNP_b</th>\n",
       "      <th>linkage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T4345C</td>\n",
       "      <td>T4288C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T4345C</td>\n",
       "      <td>T4485C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T4345C</td>\n",
       "      <td>T4280C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>T4345C</td>\n",
       "      <td>T4379C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>T4345C</td>\n",
       "      <td>T4210C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>T3670C</td>\n",
       "      <td>T3680C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>T3506C</td>\n",
       "      <td>T3513C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>T3513C</td>\n",
       "      <td>T3456C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>A8004G</td>\n",
       "      <td>A7824G</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>T8428C</td>\n",
       "      <td>T8434C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SNP_a   SNP_b linkage\n",
       "4    T4345C  T4288C       1\n",
       "5    T4345C  T4485C       1\n",
       "11   T4345C  T4280C       1\n",
       "17   T4345C  T4379C       1\n",
       "18   T4345C  T4210C       1\n",
       "..      ...     ...     ...\n",
       "578  T3670C  T3680C       1\n",
       "581  T3506C  T3513C       1\n",
       "583  T3513C  T3456C       1\n",
       "587  A8004G  A7824G       1\n",
       "588  T8428C  T8434C       1\n",
       "\n",
       "[184 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = df[df.linkage == 1]\n",
    "testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = testdf.pivot(index='SNP_a', columns='SNP_b')['linkage']\n",
    "\n",
    "x.fillna(0, inplace=True)\n",
    "result = x.combine_first(x.T).fillna(0.0)\n",
    "\n",
    "adj_matrix = [l[1:] for l in result.reset_index().values.tolist()]\n",
    "snps = result.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = {\n",
    "    i: set(num for num, j in enumerate(row) if j)\n",
    "    for i, row in enumerate(adj_matrix)\n",
    "}\n",
    "\n",
    "\n",
    "def BronKerbosch1(P, R=None, X=None):\n",
    "    P = set(P)\n",
    "    R = set() if R is None else R\n",
    "    X = set() if X is None else X\n",
    "    if not P and not X:\n",
    "        yield R\n",
    "    while P:\n",
    "        v = P.pop()\n",
    "        yield from BronKerbosch1(\n",
    "            P=P.intersection(N[v]), R=R.union([v]), X=X.intersection(N[v]))\n",
    "        X.add(v)\n",
    "\n",
    "P = N.keys()\n",
    "\n",
    "cliques = [{snps[x] for x in sets} for sets in list(BronKerbosch1(P))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "c_i = {'T6994C', 'T6936C', 'T6945C', 'T7073C'}\n",
    "c_j = {'T6994C', 'T6945C', 'T7073C'}\n",
    "\n",
    "for i in c_i:\n",
    "    for j in c_j:\n",
    "        print(result[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(testdf, source=\"SNP_a\", target=\"SNP_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500px\"\n",
       "            height=\"500px\"\n",
       "            src=\"example.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8ed901b490>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network(notebook=True)\n",
    "net.from_nx(G)\n",
    "net.show(\"example.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
